# MDP

- Markov Processes
- Markov Reward Processes(MRPs)
- Markov Decision Processes (MDPs)


## Markov Property

The future is independent of the past given the present

## Markov Process/Markov Chain

![img](https://datawhalechina.github.io/leedeeprl-notes/chapter2/img/2.5.png)

![img](https://datawhalechina.github.io/leedeeprl-notes/chapter2/img/2.6.png)

## Markov Reward Process (MRP)
- Markov Reward Process is a Markov Chain + reward

![img](https://datawhalechina.github.io/leedeeprl-notes/chapter2/img/2.8.png)

![img](https://datawhalechina.github.io/leedeeprl-notes/chapter2/img/2.9.png)

![img](https://datawhalechina.github.io/leedeeprl-notes/chapter2/img/2.10.png)

Using the Bellman Equation:
![image-20201021172235162](https://picovechou.oss-cn-shenzhen.aliyuncs.com/img/image-20201021172235162.png)

![img](https://datawhalechina.github.io/leedeeprl-notes/chapter2/img/2.13.png)

![img](https://datawhalechina.github.io/leedeeprl-notes/chapter2/img/2.14.png)

## Iterative methods for large MRPs:

- Dynamic Programming
- Monte-Carlo evaluation
- Temporal-Difference learning

### Dynamic Programming

![img](https://datawhalechina.github.io/leedeeprl-notes/chapter2/img/2.17.png)

### Monte-Carlo evaluation

![img](https://datawhalechina.github.io/leedeeprl-notes/chapter2/img/2.16.png)

## Markov Decision Processes (MDPs)

![img](https://datawhalechina.github.io/leedeeprl-notes/chapter2/img/2.19.png)

![img](https://datawhalechina.github.io/leedeeprl-notes/chapter2/img/2.20.png)

![img](https://datawhalechina.github.io/leedeeprl-notes/chapter2/img/2.21.png)

## How can be found the best policy and what are the methods?

### Policy Search

![img](https://datawhalechina.github.io/leedeeprl-notes/chapter2/img/2.43.png)

### Policy Iteration

![img](https://datawhalechina.github.io/leedeeprl-notes/chapter2/img/2.45.png)

### Value Iteration

![img](https://datawhalechina.github.io/leedeeprl-notes/chapter2/img/2.51.png)

